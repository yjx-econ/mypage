
#### News

<strong style="color:red;"><strong>I’m actively applying for a MLsys Ph.D. position in 2025 Fall! </strong></strong> If you need a student who is familiar with <strong style="color:red;"><strong>both NLP and computer systems </strong></strong> with <strong style="color:red;"><strong>extensive industry experiences</strong></strong>, feel free to <a href="#contact-info">Contact Me</a>!


#### Biography
I am currently a senior Undergraduate Student pursuing a bachelor's Degree in computer science at College of Liberal Arts, University of Minnesota Twin Cities, supervised by Prof. [Zirui Liu](https://zirui-ray-liu.github.io/). In the summer of 2023, I visited [TsinghuaNLP](https://github.com/thunlp) and conducted research under Prof. [Zhiyuan Liu](https://nlp.csai.tsinghua.edu.cn/~lzy/). 

I have experience in NLP and computer systems(both architecture and high performance machine learning systems), along with extensive industry research internship experience. This includes:

* Participating in the pretraining of the Yi-Large model at 01.AI.
* Contributing to ML Infra of the pretraining of the foundation model at ModelBest (with TsinghuaNLP)
* Participating in the finetuning of the CodeLLM [Raccoon](https://raccoon.sensetime.com/code) (Copilot-like) at SenseTime (with CUHK MMLab).

#### Research Interests

My current passion revolves around building **EFFICIENT** system solutions to AGI and LLM(VLM) for **RELIABLE** Hardware Design, this includes:

1. <strong><strong>Machine Learning System</strong></strong> 
    * Training: Design more effective training system and algorithms, examples include [BMTrain](https://github.com/OpenBMB/BMTrain).
    * Quantization
    * Long context inference: example includes [Cross Layer Attention](https://github.com/JerryYin777/Cross-Layer-Attention).
2. <strong><strong>LLM(VLM) for RELIABLE Hardware Design</strong></strong> 
    * Synthesise pretraining and finetuning common knowledge of CodeLLM, exploring the boundary capabilities of LLM/VLM for hardware design (e.g. pretrain/finetune a VerilogLLM).
    * Align the simulation code with the waveform image data to finetune VerilogVLM.

#### Misc

Before transferring to the University of Minnesota, I studied at Nanchang University, majoring in Artificial Intelligence in a top-tier class with a School Academic Special Scholarship. I was the leader of Nanchang University Supercomputer Cluster Team ([NCUSCC](https://ncuscc.github.io/)) Leader, with experience of ASC22 and SC23(IndySCC). 

I am passionate about open source and firmly believe in its potential to disseminate knowledge widely, leverage technology to lead innovation to the world and contribute to the advancement of human society. I am proud to have garnered over **1k stars** and acquired over **250 followers** on [GitHub](https://github.com/JerryYin777). I occasionally share my explorations in the machine learning system and LLM field on [Zhihu](https://www.zhihu.com/people/ycr222/posts) in Mandarin.

#### Contact<p id="contact-info"></p>

✉️ [yin00486 [at] umn.edu](mailto:yin00486@umn.edu)

<!-- Before transferring to the University of Minnesota, I studied at Nanchang University, majoring in Artificial Intelligence in a top-tier class with a School Academic Special Scholarship. I was honored to be advised by Professor [Zichen Xu](https://good.ncu.edu.cn/Pages/Professor.html) at [GOOD LAB](https://good.ncu.edu.cn) starting from March 2022, where my focus was on solving data-centric challenges and building efficient and reliable systems. I was the leader of Nanchang University Supercomputer Cluster Team ([NCUSCC](https://hpc.ncuscc.tech/)) Leader, with experience of ASC22 and SC23(IndySCC). -->

<!-- I was also fortunately recruited as a research assistant at [TsinghuaNLP](https://github.com/thunlp) in Beijing from July to September 2023, advised by Professor [Zhiyuan Liu](https://nlp.csai.tsinghua.edu.cn/~lzy/), [Weilin Zhao](https://achazwl.github.io/) and [Xu Han](https://scholar.google.com/citations?user=rz4rOSMAAAAJ&hl=zh-CN), trying to build efficient distributed large language model training framework [BMTrain](https://github.com/OpenBMB/BMTrain) and Develop 2B on-device Chinese LLM [MiniCPM](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f) at ModelBest (面壁智能). I also interned at 01.AI (零一万物) and SenseTime Research (商汤研究院) as algorithm intern. -->

<!-- I am passionate about open source and firmly believe in its potential to disseminate knowledge widely, leverage technology to lead innovation to the world and contribute to the advancement of human society. I am proud to have garnered over **1000 stars** and acquired over**200 followers** on GitHub. It is gratifying to know that my open-source projects have benefitted numerous individuals, and I have personally gained valuable knowledge from the open-source community. -->

<!-- #### Contact
* Email: yin00486 [at] umn.edu

#### Education
2023.12 - Present, Computer Science, College of Liberal Arts, University of Minnesota Twin Cities.

2021.09 - 2023.12, Artificial Intelligence, School of Information Engineering, Nanchang University.

#### Research Interests
Natural Language Processing, Machine Learning System.

#### Skills
* **Natural Language Processing:** Proficient in using the PyTorch framework, with the ability to reproduce mainstream large-scale models in the industry (such as Baichuan, llama2, Qwen). Proficient in using quantization and inference tools such as QLoRA, vLLM, and skilled in distributed parallel training (using training tools such as BMTrain, DeepSpeed).

* **High Performance Computing:** Proficient in CUDA Programming, familiar with C++, knowledgeable in compiler optimization principles, and understanding of MPI, OpenMP, and SIMD acceleration optimization technologies.

* **Computer System Architecture:** Familiar with GPU architecture and RISC-V instruction set, and has participated in the [One Life, One Core](https://ysyx.oscc.cc/) project.

* **Other:** Understanding of serverless computing architecture, experience in applying federated learning in network security, involvement in both front-end and back-end development, familiarity with Linux operating system and operation and maintenance ( maintained a large server cluster with 21 nodes in the GOOD LAB). Additionally, has researched computer vision and reinforcement learning in the field of artificial intelligence. -->
